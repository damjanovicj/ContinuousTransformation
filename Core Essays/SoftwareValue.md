cross-posted at https://medium.dromologue.com

# What Drives Software Value?

There is a prevailing wisdom of how to build great software today. The results are spotty though, if we are honest, as the implementation of that wisdom can be inconsistent and lacking integrity to the core ideas themselves. I know of companies that decide to train 1000 people in ‘Agile’ and wonder why they aren’t ‘Agile’ a few months later, or why the word engenders eye-rolls when mentioned at the water-cooler. A narrow interpretation of DevOps as merely the de-siloing of dev and ops often produces the challenge from IT veterans that, “that was how we used to do it.” But there is also the danger of demarcation disputes breaking out among purists. Does Continuous Delivery not imply ‘DevOps’? Martin Fowler (ContinuousDelivery — Martin Fowler) would say that high velocity testing, integration and delivery certainly is enabled by the same behaviours. Both purity and impurity have their risks, it seems.
Whatever the detail in the canon, I think there are 5 key ideas that summarise the outcomes all of these approaches attempt to satisfy:
+ Velocity → delivering something at high speed
+ Granularity → delivering smaller pieces in order that they may be tested
+ Density → ensuring resources work better together
+ Concreteness → delivering concrete actionable value
+ Agency → enabling options, choice and freedom to act.

## Velocity
At its core, Continuous Delivery is about delivering working code as frequently as possible. Deferring integration merely defers confirmation of the quality of the software produced. Similarly, deferring release to customers defers validation of the fit of your software and dampens your ability to get and respond to feedback from your users. However, there is also an implication, sometimes missed, for the practise of design or architecture. Assuming frequent delivery of ‘value’, and the refinement of the proposition over time, promotes better design by emphasising modularity, YAGNI (you aint gonna need it) principles and flatter interfaces / APIs.
## Granularity
From an organisational perspective, velocity is critical for any enterprise with significant technical debt. Rather than a currently vogue definition of technical debt as ‘untested code’ I prefer ‘untestable systems’ as a definition. These are ignored, largely untested and now essentially untestable due to lack of documentation, the blackbox nature of the systems and the irreversibility of their build. The only way to reclaim these systems is through incremental replacement of granular functions / packages / modules with test automated components over time. Velocity requires the delivery of granular features whose expected behaviour is well defined, test automated and with well understood dependencies. Similarly, granularity of delivery also entails better understanding across the whole team of the ‘detail’ of the features being deployed. This in turn enables better definition of what value is driven by the delivery of these features. In the next section where we discuss value, the need to tie clear hypotheses of the value to be delivered by each feature will be discussed in more detail.
## Density
Density in this definition refers to squeezing more value into / or out of a set of resources. Add to this the need for these resources’ effectiveness to improve continuously over time and we hit a core idea within the lean movement. The Poppendiecks are good here (Friction). Lean principles aim to remove the barriers that slow down or add waste to the production of end customer value. However, the same focus on creating highly collaborative teams that have shared metaphors and an obsessive focus on the chain of value from development to deployment is also found in Agile and DevOps (Chef Style DevOps Kungfu — Adam Jacob Keynote …) descriptions today. The main point being, without a focus on the improvement of both the amount of value created by a team as well as the value delivered by each individual feature deployed, competitiveness declines as well as creativity in those teams finding themselves doing the same thing with different labels over and over again. While the term, devops, is unfortunate in that it tends to over signify the relative importance of dev and ops collaboration it is a good example of how density is achieved. The refuting of Conway’s Law by breaking the barriers that impede knowledge and accountability sharing across development and operations teams (perhaps by collapsing them together) is critical in ensuring that we produce systems that are as readily extended as they are supportable.
## Concreteness
The agile mindset, so well described by Linda Rising (The Power of an Agile Mindset — Linda Rising — SlideShare), is inclusive, enquiring and expansive. We seek ways to grow and to improve. That is not necessarily to say though that pre-agile teams do not also have those values nor indeed that teams self-identifying as agile do. There is behaviour inherent in successful agile teams that chooses to resolve abstract questions of priority, design and implementation through action. Note that this also entails granular knowledge of what questions need to be answered, and that these questions will be improved by dense teams, but the awareness that nothing tests theory better than action is what enables a fast progression from questioning to resolving. If we consider what makes post-mortems and retrospectives successful, it is the construction and questioning of concrete causes and effects. X led to Y… we think this was because Z. How could we validate that idea about Z?
## Agency
The density requirement can easily lead to a cynical interpretation of how to generate more productivity. Like Frederick Winslow Taylor and his Hungarian workers (Taylorism and the Workers at Bethlehem Steel, 1898–1901). The freedom to exercise creativity in the solution to problems, to control process in a manner that suits the team and their value objectives is critical to sustained improvement because without it, the productivity gains will only ever be those that are expected and plannable, narrowly defined within the available levers controllable by management. These are never the same levers, nor as effective as the teams own prioritisation of conditions for their success.
## ALDO
The prevailing wisdom then proposes the following as an outcome to be sought; I summarise it as ALDO — Agile, Lean, DevOps Outcomes:
“The release of software at high velocity, enabled by development in small increments, the success of which the entire team may understand such that they may make subsequent priority decisions based on validated hypotheses rather than supposition. In the process of delivering software, in order for its continued improvement, the team must also focus on the conditions for its improvement as a team.”
But do these attributes of successful software delivery explain why software as an economic entity is as successful as it is?
Friction
Consider a simple example. Two decades ago, you would choose a car based on your ideal product characteristics (make, size etc.) and how much you could afford to spend. Your choice would live with you while you paid it off, or until you could justify changing it. The idea of choosing products based on product features is, of course, at the core of all consumer choice. If this weren’t true, how would we decide what to buy?
But what happens when we relax the assumption that you have to live with your choice for some period of time? What would happen to your choice of car if any choice you made today could be rescinded tomorrow and a new choice made? What if your car was updated every week instead of every year? What if there was an April 2016 model of the new Ford instead of just the 2016 model?
Well, the customer now has the opportunity to make feature choices at a greater frequency than has was possible in the past. And this is actually beginning happen at the software level as car operating systems are updated wirelessly and provide new features — through software updates — to existing owners. In this case, the car becomes a software platform, or rather a platform for software. The physical choice of platform (car) is also beginning to change, though. For those who wish to change their car regularly, there are many services (e.g. Zip Car) that allow you to rent access to a car and return it (or just leave it somewhere) when you’re done.
The contrasting cases of cars being easily swapped when you don’t want to own one and the case of a car you own being software-updated regularly are instructive: They both aim to reduce the friction of making changes. The former, of the physical product itself; and the latter, the speed of added adding new features, independent of the physical platform itself. But in both cases, these features are accessed using some software — car operating system or car app.
Imagine if, for a particular piece of software, say a task management application, you could get it for nothing (or at a very low price) — such that changing it becomes unconstrained by its cost to you. Add to this that new features are being bolted on to the application, and its competitors’ products too, very frequently. The only feature limiting your ability to move to another application is the application’s ability to let you transport your personalization data (e.g. your tasks in the task app) to the new app. If your requirements changed very slowly, and other products changed very slowly, it is likely — as in the past — that the value of switching would be low. But if you were constantly on the lookout for new features and wanted to try them out, and new features were continuously being added, it follows that every app you chose to move to would need to enable you to move “back” again. This is because the prevailing assumption of the software product consumer is that other applications will conceivably meet their needs better in future.
Obviously, Every software product has a set of threshold features without which consumers will tend not to choose it. A car needs to be waterproof (!) with airbags and ABS brakes. A task app needs to allow you to add dates to your tasks. This set of features starts off as what is often termed a minimum viable product. However, my thesis is that , this set now includes the ability to switch away from the product and back again. Software products are being designed to reduce utility friction.
## Value
In medieval times, the value of a good was thought to consist in its utility, or usefulness and it’s scarcity. So, wood in deserts would be more valuable than wood in rainforests. The problem with this view was that it did not take into account what it cost to produce something. Given a marketplace full of wooden wagons, which wagon would have the most value?
In the 17th century, theorists like Petty and Cantillon tried to solve this problem by rooting value in the basic factors of production — labour and land respectively. Some wood is harder to chop than others (apparently). Philosophers like John Locke however countered that utility remained a decisive factor following an earlier philosopher, John Law, who coined the water diamond paradox — Diamonds have no utility to a parched man in the desert.
Notwithstanding this, throughout most of the 18th and 19th century economists adopted a labour centrered theory of value. Politically and socially this made sense as land became claimed for industrial scale development and skilled tradesmen from the country were drafted in to work in factories. How much should they be paid? Karl Marx famously claimed that ‘All commodities are only definite masses of congealed labour time.’ Capital, like machines (and software much later) locked up value and released it for those who owned the capital rather than those who made it, the labourer. The problem with labour and land based theories of production though, was that they did not explain why prices did not go up even for goods where regardless of demand increases, supply always stayed the same — for instance, a cooper can only make so many barrels.
In the 19th Century, Jevons and Menger revisited the utility theory of value, independently claiming that all value was based on utility. Jevons’ quote outlines the rather massive shift now being considered, “Cost of production determines supply, supply determines final degree of utility, final degree of utility determines value.” This was the so-called ‘marginalist’ revolution.
We finally arrive in the 20th Century to work done by Alfred Marshall. He attempted to reconcile the labour and utility theories of value by considering ‘time effects’ — i.e. the time each factor takes to have an impact on the final value — of each.
So we find ourselves with a basic model of value that is determined by the factors of production (land, labour and Capital), utility and time.
## Putting it Together
1. Granularity , Density and Concreteness as attributes of software value cohere well with the understanding of which factors drive the price of software production and how they may be managed effectively.
We find granular design decisions at the heart of product choice today.. The ability to customise your car, your clothes sizing etc. Is the basis of differentiation. Software just enables this value attribute infinitely more. Similarly, where density runs up against capital requirements — such as the cost of replacing one robotic car production line with a better one (more arms?) — we see that software, with its value based on understanding of the technology and requirements, enables lower cost improvement. A commodity, such as gold, steel or corn, is the most basic concrete measure of value. Commodities are a safe haven that software perhaps will never be. However, the volatility in software company valuation should not be confused with the value of software itself. Its ability to persist is based purely on its continuous delivery of concrete value to customers, who more and more choose it as a mechanism through which to consume other services (such as commodity trading.).
2. Velocity, applies directly to the insight that value has a time component.
The fungible aspect of software — the fact that it doesn’t physically degrade and may be consumed infinitely — creates what is essentially a distinct factor of production. Software is a crystallisation of labour knowledge into a form of capital that is value preserving ( like gold) but whose value declines faster and which becomes obsolete much faster than physical capital. We see this, in the choice to lengthen the life of major capital equipment by offering software upgrades instead of requiring full capital replacement. As Joseph Schumpeter predicted, the continual ‘destruction’ of value through obsolescence fuels innovation. We see this nowhere more clearly than in the software economy today. The metaphor of Friction is apposite. Anything that slows down this revaluation / redevelopment and reiteration of software destroys value.
3. Agency, is an attribute of utility for both software engineers, choosing to labour on your project and the customer perceiving greater optionality provided by the software for the the jobs they need done.
If optionality is core to perceived utility then the ability of users to exercise those options must be core too. What is unique in knowledge based businesses is the focus on enabling the creativity of labour itself. That would be us. The creation of self-organising, self-prioritising teams puts the onus for responding to events on the team, which is where it belongs since they are closer to its occurrence than any supervisor. This unique aspect, drives innovation in the product itself and of the production process itself creating a surplus value over and above that which is achievable in commodity or static goods.
